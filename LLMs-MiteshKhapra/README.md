# Introduction to Large Language Models by Mitesh Khapra

## Part 1

### Week 1	
#### A deep dive into the components of a transformer architecture
#### A discussion on the the computational complexity of vanilla transformers
### Week 2	
#### What is a language model?
#### Decoder-only LLMs: A deep dive into GPT to understand the architecture and training objectives (Causal language model)
#### The distinction between pre-training and fine-tuning
#### Understanding decoding strategies
### Week 3	
#### Encoder-only LLMs: A deep dive into BERT to understand the architecture and training objectives (Masked Language Model)
#### Understanding tokenization (BPE)
### Week 4	
#### Encoder-Decoder models: A deep dive into T5 text-to-text to understand the architecture and training objectives
#### Understanding the idea of a prompt
### Week 5	
#### The bigger picture: A taxonomy of all models (encoder-only, decoder-only, encoder-decoder only)
#### What do they differ in? Data (sources and pipelines), Model (scale, attention, PEs, vocab), Training (optimiser, objective functions)
